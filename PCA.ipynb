{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGAKAkms6s+Aj38mrXA4G3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miloszpiechota/AI-Systems/blob/main/PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF4XRL7VgA4J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head()\n",
        "df.drop('Tm',axis=1,inplace=True)\n",
        "X=df.iloc[:,0:27]\n",
        "y = df.iloc[:,27]\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=21)\n",
        "scaleStandard = StandardScaler()\n",
        "X_train = scaleStandard.fit_transform(X_train)\n",
        "X_test = scaleStandard.transform(X_test)\n",
        "df.columns\n",
        "\n",
        "X_train=pd.DataFrame(X_train,columns=['A','B','C'])\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "pca1=PCA()\n",
        "X_pca1 = pca1.fit_transform(X_train)\n",
        "pca1.explained_variance_ratio_\n",
        "plt.plot(pca1.explained_variance_ratio_)\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.show()\n",
        "\n",
        "pca2=PCA(0.95)\n",
        "X_pca2 = pca2.fit_transform(X_train)\n",
        "X_pca2.shape\n",
        "pca2.explained_variance_ratio_\n",
        "\n",
        "pca2c = PCA(n_components=2)\n",
        "X_pca2c = pca2c.fit_transform(X_train)\n",
        "\n",
        "colormap=plt.cm.get_cmap('coolwarm')\n",
        "plt.figure()\n",
        "scatter = plt.scatter(X_pca2c[:,0],X_pca2c[:,1],c=y_train,cmap=colormap)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.colorbar(scatter)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "feature_names_file = 'features.txt'\n",
        "activity_labels_file = 'activity_labels.txt'\n",
        "train_data_file = 'train/X_train.txt'\n",
        "train_labels_file = 'train/y_train.txt'\n",
        "train_subjects_file = 'train/subject_train.txt'\n",
        "test_data_file = 'test/X_test.txt'\n",
        "test_labels_file = 'test/y_test.txt'\n",
        "test_subjects_file = 'test/subject_test.txt'\n",
        "\n",
        "# Load feature names\n",
        "feature_names = pd.read_csv(feature_names_file, delim_whitespace=True, header=None, usecols=[1])\n",
        "feature_names = feature_names[1].tolist()\n",
        "\n",
        "# Load activity labels\n",
        "activity_labels = pd.read_csv(activity_labels_file, delim_whitespace=True, header=None, index_col=0)\n",
        "activity_labels_dict = activity_labels[1].to_dict()\n",
        "\n",
        "# Load training data\n",
        "X_train = pd.read_csv(train_data_file, delim_whitespace=True, header=None)\n",
        "y_train = pd.read_csv(train_labels_file, delim_whitespace=True, header=None)\n",
        "subject_train = pd.read_csv(train_subjects_file, delim_whitespace=True, header=None)\n",
        "\n",
        "# Load test data\n",
        "X_test = pd.read_csv(test_data_file, delim_whitespace=True, header=None)\n",
        "y_test = pd.read_csv(test_labels_file, delim_whitespace=True, header=None)\n",
        "subject_test = pd.read_csv(test_subjects_file, delim_whitespace=True, header=None)\n",
        "\n",
        "# Combine training and test data\n",
        "X = pd.concat([X_train, X_test], ignore_index=True)\n",
        "y = pd.concat([y_train, y_test], ignore_index=True)\n",
        "subjects = pd.concat([subject_train, subject_test], ignore_index=True)\n",
        "\n",
        "# Assign column names\n",
        "X.columns = feature_names\n",
        "y.columns = ['Activity']\n",
        "subjects.columns = ['Subject']\n",
        "\n",
        "# Map activity labels\n",
        "y['Activity'] = y['Activity'].map(activity_labels_dict)\n",
        "\n",
        "# Combine all data into a single DataFrame\n",
        "data = pd.concat([subjects, y, X], axis=1)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Remove outliers using Z-score\n",
        "z_scores = np.abs(stats.zscore(data.iloc[:, 2:]))\n",
        "data = data[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "# Remove low-variance features\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_reduced = selector.fit_transform(data.iloc[:, 2:])\n",
        "selected_features = data.iloc[:, 2:].columns[selector.get_support(indices=True)]\n",
        "data_reduced = pd.DataFrame(X_reduced, columns=selected_features)\n",
        "data = pd.concat([data.iloc[:, :2].reset_index(drop=True), data_reduced], axis=1)\n",
        "\n",
        "# Remove highly correlated features\n",
        "corr_matrix = data.iloc[:, 2:].corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
        "data.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(data.iloc[:, 2:])\n",
        "data_scaled = pd.DataFrame(features_scaled, columns=data.columns[2:])\n",
        "data_scaled = pd.concat([data.iloc[:, :2].reset_index(drop=True), data_scaled], axis=1)\n",
        "\n",
        "# Save the cleaned and standardized data\n",
        "data_scaled.to_csv('cleaned_data.csv', index=False)\n",
        "\n",
        "# Apply PCA to retain 95% of variance\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "principal_components = pca.fit_transform(data_scaled.iloc[:, 2:])\n",
        "pca_columns = [f'PC{i+1}' for i in range(principal_components.shape[1])]\n",
        "pca_data = pd.DataFrame(principal_components, columns=pca_columns)\n",
        "pca_data = pd.concat([data_scaled.iloc[:, :2].reset_index(drop=True), pca_data], axis=1)\n",
        "\n",
        "# Save the PCA-transformed data\n",
        "pca_data.to_csv('pca_data.csv', index=False)\n",
        "\n",
        "print(\"Data preprocessing complete. 'cleaned_data.csv' and 'pca_data.csv' have been saved.\")\n"
      ],
      "metadata": {
        "id": "GGBaj9VwwWU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "syk9DVmbx8vH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}